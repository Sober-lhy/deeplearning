{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from blackd.middlewares import F\n",
    "####################小土堆pytorch深度学习学习笔记##################\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,root_dir, label_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(self.root_dir,self.label_dir)\n",
    "        self.imgs = os.listdir(self.path)\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        img_path = os.path.join(self.path,img_name)\n",
    "        img = Image.open(img_path)\n",
    "        label = self.label_dir\n",
    "        return img, label\n",
    "\n",
    "#已经构建了数据集的类\n",
    "#下面就是设计自己的数据集传参数\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334, 500, 3)\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "writer = SummaryWriter('logs')\n",
    "img_path = \"hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg\"\n",
    "img_PIL = Image.open(img_path)\n",
    "img_array = np.array(img_PIL)\n",
    "print(img_array.shape)                    # .shape 看看某个元素的格式\n",
    "\n",
    "\n",
    "writer.add_image(\"bees/21399619\", img_array, 0, dataformats='HWC')   # 看看某个函数的的参数\n"
   ],
   "id": "ef0a2d86b2e7b5b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T02:40:43.565032Z",
     "start_time": "2024-08-08T02:40:43.561707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 学到一些东西\n",
    "# writer.add_image是把图片绘制到 tensorboard上的函数\n",
    "# 用tensorboard --logdir=logs打开所在的tensorboard\n",
    "# img_array.shape是用来查看这个元素的规格的\n",
    "# 调用函数可以查文档 看看需要什么样子的参数\n",
    "# 下午一来就练习：怎么把图片搞到tensorboard上面\n"
   ],
   "id": "91126a9f41f66401",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T03:06:23.198131Z",
     "start_time": "2024-08-08T03:06:23.177310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transform的使用 transform是对图片的格式进行变换\n",
    "from torchvision import transforms\n",
    "# transform的作用就是把一个图片输出成为我们想要的格式类型\n",
    "# ex\n",
    "# 1.transforms该怎么去使用?\n",
    "# 2.To tensor（）以及tensor张量该怎么使用?\n",
    "img_path = \"hymenoptera_data/train/bees/90179376_abc234e5f4.jpg\"\n",
    "img_PIL = Image.open(img_path)\n",
    "tensor_trans=transforms.ToTensor() #类的调用 要先创建一个类对象 然后使用类里的方法或者全局变量\n",
    "img_tensor=tensor_trans(img_PIL)\n",
    "print(type(img_tensor))\n",
    "print(img_tensor.shape)"
   ],
   "id": "e7efc6c358c67429",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 332, 500])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T01:38:49.218218Z",
     "start_time": "2024-08-09T01:38:48.970470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pIL image是一种图片的处理方式\n",
    "# opencv 也是一种图片的处理方式\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "Writer=SummaryWriter('logs')\n",
    "\n",
    "img_path = \"hymenoptera_data/train/ants/460372577_f2f6a8c9fc.jpg\"\n",
    "img_PIL = Image.open(img_path)\n",
    "\n",
    "tensor_trans=transforms.ToTensor()\n",
    "img_tensor=tensor_trans(img_PIL)\n",
    "Writer.add_image(\"蚂蚁2\", img_tensor, 0)  # tensorboard可以打开张量和数组形式的图片\n",
    "img_tensor.shape"
   ],
   "id": "a062c98d76f6dbb1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 407, 500])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T02:27:53.878562Z",
     "start_time": "2024-08-09T02:27:53.874021Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b7e30bc9f4a6ebe2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T02:32:11.131827Z",
     "start_time": "2024-08-09T02:32:10.908910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transforms的使用\n",
    "# 图片的各种格式  PIL  image.open(\"路径\")\n",
    "# tensor  可以把PIl图片转化成为tensor格式 进行深度学习\n",
    "# cv2也有图片操作的东西 cv.imread()\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "Writer=SummaryWriter('logs')\n",
    "img_path = \"hymenoptera_data/val/bees/abeja.jpg\"\n",
    "img_PIL = Image.open(img_path)\n",
    "#transforms.totensor\n",
    "tensor_trans = transforms.ToTensor()\n",
    "img_tensor=tensor_trans(img_PIL)\n",
    "# 以下是把图片标准化 便于处理的一种方式 transforms.\n",
    "trans_normal=transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "# 1.RGB图片是三通道【】内有三项  2.前【】是均值后【】是方差\n",
    "img_tensor_normal=trans_normal(img_tensor)\n",
    "Writer.add_image(\"bees/abeja\", img_tensor_normal)\n",
    "Writer.add_image(\"2\",img_tensor)\n"
   ],
   "id": "953b08da327956c6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T02:55:25.745565Z",
     "start_time": "2024-08-09T02:55:18.320342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transforms 中的resize的使用 resize的使用方法\n",
    "\n",
    "from torchvision import transforms\n",
    "print(img_PIL.size)\n",
    "img_PIL.show()\n",
    "tran_resize=transforms.Resize((224, 224))  # resize里面需要的是PIL格式的图片\n",
    "img_PIL_resize=tran_resize(img_PIL)        \n",
    "img_PIL_resize.show()"
   ],
   "id": "bb9b3e1ebe017b88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 300)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "做一个小总结：\n",
    "现在已经学习了\n",
    "1，数据集的调用\n",
    "1）其中类的调用方式（构造函数）传递构造函数的参数值 直接调用类\n",
    "2）os.join()这个函数对地址进行拼接\n",
    "3）os.listdir()获得该目录下文件地址的列表\n",
    "2.tensorboard的使用\n",
    "1）调用方法   form torch.utils.tensorboard import summary writer\n",
    "2)writer=summarywriter(\"文件名\")   writer.add_image()  参数：名字   tensor/numpy  step  格式\n",
    "3）logdir=logs\n",
    "3.transforms的使用\n",
    "1）transform_trans=transforms.totensor(）  创建了一个totensor的对象\n",
    "2）transform_normal=transforms.normalize([0.5,0.5,0.5],[0.5,0.5,0.5])均值和方差（RGB图片的三通道）  创建了一个normal的对象\n",
    "作用是标准化  参数： 只能是tensor格式的张量 是改变图片的内部RGB参数 \n",
    "3）transform_resize=transforms.resize((210,210))  参数PIL格式的图片   是改变图片的物理尺寸   \n",
    "'''"
   ],
   "id": "fd8570070dcc5e0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#transforms=transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
    "#如上图1.compose的参数需要是列表的形式【listnum1，listnum2，……】\n",
    "#     2.列表中的元素需要是transforms类型\n",
    "# Randomcrop（） transforms.CenterCrop()、‌transforms.ToTensor()和transforms.Normalize()\n",
    "# 0.随机裁剪1.中心裁剪2.to张量 3.标准化\n",
    "# 要自己学会怎么去查一个函数的作用 以及使用方法  这个是学习python的核心思想。\n"
   ],
   "id": "c4c9e99aeb425849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T13:24:40.005698Z",
     "start_time": "2024-08-09T13:24:38.637004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#如何把transforms和数据集torchvision结合起来使用 批量处理图片数据\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch. utils. tensorboard import SummaryWriter\n",
    "dataset_transforms=transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR10(root=r\"D:\\我的texts\", train=True,\n",
    "                                         transform=dataset_transforms, download=False)\n",
    "text_set = torchvision.datasets.CIFAR10(root=r\"D:\\我的texts\", train=False,\n",
    "                                        transform=dataset_transforms, download=False)\n",
    "# 数据集里面的参数是text_set[0].shape\n",
    "writer=SummaryWriter('logs')\n",
    "for i in range(10):\n",
    "    img, label = train_set[i]\n",
    "    #train_set[]是一个元组  但可以把这个元组的数据依次赋给元素\n",
    "    writer.add_image(\"train\", img, i)\n",
    "# for循环 用tensorboard打开数据集里的所有图片            transforms and torchvision and tensorboard的综合使用\n",
    "writer.close()\n",
    "#只有张量的形式有   .shape\n",
    "# root= 意思是要把数据集存在什么地方(这个文件夹的前一级目录)  train的意思是  \n",
    "# transforms数据类型 对所有元素进行修改    download是是否下载"
   ],
   "id": "df5573e8cb7d06b1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:41:02.588218Z",
     "start_time": "2024-08-12T09:40:04.988951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dataset 和 dataloader的区别\n",
    "# dataloader就是要把数据从dataset 里面把数据放到神经网络中去\n",
    "# dataloader  数据加载模块 可以控制怎么去取数据\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch. utils. tensorboard import SummaryWriter\n",
    "transforms=transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "text_dataset = torchvision.datasets.CIFAR10(root=R'D:\\我的texts', train=True,\n",
    "                                            transform=transforms, download=False)\n",
    "text_loader=DataLoader(text_dataset,batch_size=64,shuffle=True,num_workers=0,drop_last=True)#打包带走的图片\n",
    "writer=SummaryWriter('logs')                                                              #drop_last是否舍去最后\n",
    "step=0                                           #shuffle就是确定不同轮次的取图片顺序是否相同       \n",
    "# epoch可以确定不同轮次的取图片\n",
    "for data in text_loader:\n",
    "    imgs, labels = data\n",
    "    writer.add_images(\"text\", imgs, step)    #writer.add_images 和 add_image 是不一样的   \n",
    "    step=step+1\n",
    "\n",
    "#  print(imgs.shape)\n",
    "# print(labels.shape)\n",
    "#需要写一个for循环在text_loader上面\n",
    "# text_dataset返回的是图片和标签  要用img，label=text_set[0]来接收返回值\n",
    "# text_loader的作用就是（例子batch_size=4）就是把text_set[0],1,2,3 四个元素的img和label进行整体的打包也\n",
    "# 也就是把前四个元素进行了一个打包"
   ],
   "id": "8fba19416046872e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 关于proch里面神经网络的搭建 是pytorch里面的核心内容\n",
    "from torch import nn\n",
    " #神经网络所有的骨架\n",
    "'''\n",
    " class Model(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 20, 5)     卷积\n",
    "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.conv1(x))            卷积之后用 relu激活函数\n",
    "                return F.relu(self.conv2(x))         同样返回一次卷积和激活函数之后的结果\n",
    "'''\n"
   ],
   "id": "b4ecf8622ab5c799"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:48:51.287518Z",
     "start_time": "2024-08-10T07:48:47.005462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 开始构建自己的神经网络\n",
    "from torch import nn\n",
    "import torch\n",
    "class My_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input1):\n",
    "         #input就是数据集输入  怎么利用这个input把图片数据集导入进来\n",
    "        output=input1+1     #定义自己的神经网络\n",
    "        return output\n",
    "\n",
    "model = My_nn()     #创建类对象  和dataset是一样的方式\n",
    "x = torch.tensor(1.0) #这个就是input           \n",
    "y = model(x)          #这个就是output   \n",
    "print(y)\n",
    "print(y.shape) \n",
    "#实际上的神经网络就是要   1. 写正向传播的方法  定义不同的参数即在forward里面写明白正向传播的方式\n",
    "                    # 2. input数据集要用合理的方式传入  例如要传入图片数据集 用到transform对数据集进行处理\n",
    "                       #  也可以使用class（dataset）创建自己的数据集类对象\n",
    "                    # 3.再调用神经网络类 得到最后的output\n",
    "                    # 4.可以使用 tensorboard对结果进行展示   或者利用MATLAB的包也可以展示"
   ],
   "id": "be675427690c10ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T08:12:52.847407Z",
     "start_time": "2024-08-10T08:12:52.839129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "#nn.Conv2d  这个就是对于2维元素进行的卷积操作\n",
    "import torch\n",
    "\n",
    "input=torch.tensor([[1,2,0,3,1],\n",
    "              [0,1,2,3,1],\n",
    "              [1,2,1,0,0],\n",
    "              [5,2,3,1,1],\n",
    "              [2,1,0,1,1]])\n",
    "kernel=torch.tensor([[1,2,1],\n",
    "                     [0,1,0],\n",
    "                     [2,1,0]])\n",
    "\n",
    "print(input.shape)\n",
    "input=torch.reshape(input,(1,1,5,5))      #重构reshape 重新设置tensor的格式来满足不同函数对于参数的需求\n",
    "kernel=torch.reshape(kernel,(1,1,3,3))\n",
    "print(input.shape)\n",
    "#这样就可以简单验证 nn.conv2d()二维卷积的使用"
   ],
   "id": "c4ef66f5fba9d7f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "902884faa9c6bd62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T10:20:03.513238Z",
     "start_time": "2024-08-12T10:20:03.274953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, label):\n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.path = os.path.join(self.root, self.label)\n",
    "        self.imgs = os.listdir(self.path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        img_path = os.path.join(self.path, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        label = self.label\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "root_dir = \"hymenoptera_data/train\"\n",
    "label_dir = \"ants\"\n",
    "\n",
    "dataset = MyDataset(root_dir, label_dir)\n",
    "\"\"\"\n",
    "while dataset.label == \"ants\":\n",
    "    img, label = dataset[i]\n",
    "    i += 1\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  #不能直接用totensor 必须用compose才能处理批量数据\n",
    "    img = transform(img)\n",
    "\"\"\"\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "for data in dataloader:\n",
    "    imgs, labels = data\n"
   ],
   "id": "ae44681136ee472",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 40\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03mwhile dataset.label == \"ants\":\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;124;03m    img, label = dataset[i]\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m    img = transform(img)\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     39\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m     41\u001B[0m     imgs, labels \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:191\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m             \u001B[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001B[39;00m\n\u001B[0;32m    188\u001B[0m             \u001B[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001B[39;00m\n\u001B[0;32m    189\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem_type))\n",
      "\u001B[1;31mTypeError\u001B[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T05:21:26.483313Z",
     "start_time": "2024-08-13T05:21:18.750353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#终于开始干神经网络   torch.nn的moudle的问题\n",
    "####################################torch.nn pytorch的神经网络的问题#####################################\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = torchvision.datasets.CIFAR10(root=\"D:\\我的texts\", train=True,transform=transform, download=False)\n",
    "dataloader=DataLoader(dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "writer=SummaryWriter('logs')\n",
    "#开始今天的重点\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3,stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "step = 0\n",
    "My_net=Net()    #  神经网络的初始卷积层的问题\n",
    "for data in dataloader:\n",
    "    img1, labels = data\n",
    "    outputs = My_net(img1)\n",
    "    print(outputs.shape)\n",
    "    writer.add_images(\"input\", img1, step)\n",
    "    outputs = torch.reshape(outputs, (64, 3, 30, 30))\n",
    "    writer.add_images(\"output\",outputs,step)\n",
    "    step=step+1"
   ],
   "id": "1b3e0ac353d6d146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 30, 30])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 3, 30, 30]' is invalid for input of size 345600",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     30\u001B[0m writer\u001B[38;5;241m.\u001B[39madd_images(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m, img1, step)\n\u001B[1;32m---> 31\u001B[0m outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mreshape(outputs, (\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m3\u001B[39m, outputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], outputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m]))\n\u001B[0;32m     32\u001B[0m writer\u001B[38;5;241m.\u001B[39madd_images(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m,outputs,step)\n\u001B[0;32m     33\u001B[0m step\u001B[38;5;241m=\u001B[39mstep\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[64, 3, 30, 30]' is invalid for input of size 345600"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#########最大池化层的问题\n",
    "#nn.MaxPool2d()"
   ],
   "id": "8f35a56bd42babeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
